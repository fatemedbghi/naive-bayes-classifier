{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive‬‬ ‫‪Bayes‬‬ ‫‪Classifier‬‬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fateme Seyyed Dabbaghi 810197529\n",
    "#### In this project we are going to analyze the comments of products of an online market to guess whether the product is recommended or not based on each comment using naive bayes classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to achieve our goal we have two datas; train data and test data. using train data after merging each title and comment of a specific comment we calculate the probability of each word appearing in category \"recommended\" and \"not recommended\". then for each comment of test data we calculate the total probability by Bayes rule for both categories. by comparing those two probabities we decide whether the comment belongs to first category or the second.we use hazm library for normalizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv ('comment_train.csv')\n",
    "test_data = pd.read_csv ('comment_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 3 steps down here for normalizing data:\n",
    "<br>1) normalize: this function is used for replacing space with half-space if necessary. this function makes data integrated so that data proccesing will be easier.\n",
    "<br>2) stemming: stemming is the process of producing morphological variants of a root/base word. but it may be wrong in some cases like when two words are stemmed from the same root that are of different stems. while calculating the probability of each word this method helps in a way that it turns all different words made by unique root to root so that the probabilities will be more accurate.\n",
    "<br>3) ‫‪lemmatization‬‬: The goal of lemmatization same as stemming is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "<br>Stemming usually refers to a  process that chops off the ends of words in the hope of achieving this goal correctly most of the time. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word. If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_train = train_data[(train_data['recommend'] == 'recommended')]\n",
    "recommend_count = recommend_train.count()['title']\n",
    "not_recommend_train = train_data[(train_data['recommend'] == 'not_recommended')]\n",
    "not_recommend_count = not_recommend_train.count()['title']\n",
    "\n",
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def normalize_str(str):\n",
    "    data = word_tokenize(normalizer.normalize(str))\n",
    "    for i in range(len(data)):\n",
    "        data[i] = lemmatizer.lemmatize(stemmer.stem(data[i]))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_freq(data):\n",
    "    count = dict(collections.Counter(data))\n",
    "    return count    \n",
    "\n",
    "def normalize_data(flag):    \n",
    "    rec_words = []\n",
    "    not_rec_words = []\n",
    "\n",
    "    for index,row in recommend_train.iterrows():\n",
    "        if flag == 1: rec_words += normalize_str(row['title'] + ' ' + row['comment'])\n",
    "        else: rec_words += word_tokenize(row['title'] + ' ' + row['comment'])\n",
    "        \n",
    "    for index,row in not_recommend_train.iterrows():\n",
    "        if flag == 1: not_rec_words += normalize_str(row['title'] + ' ' + row['comment'])\n",
    "        else: not_rec_words += word_tokenize(row['title'] + ' ' + row['comment'])\n",
    "        \n",
    "    rec_data = calc_freq(rec_words)\n",
    "    not_rec_data = calc_freq(not_rec_words)\n",
    "\n",
    "    test = test_data.copy()\n",
    "\n",
    "    for index,row in test.iterrows():\n",
    "        if flag == 1: temp = normalize_str(row['title'] + ' ' + row['comment'])\n",
    "        else: temp = word_tokenize(row['title'] + ' ' + row['comment'])\n",
    "        for i in temp:\n",
    "            if i not in rec_data: rec_data[i] = 0\n",
    "            if i not in not_rec_data: not_rec_data[i] = 0\n",
    "                \n",
    "    return rec_data,not_rec_data,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we calculate the probability of each word by counting the occurrences of it in train data. we may not have specific word that accured in comment in test data in one or both categories. as we multiply the probabilies this leads the probability of comment to zero and its just one word and we could guess the comment category by other words. or in another case when we have that word in one of the categories we say for sure that the comment belongs to that category because the other probability is zero.\n",
    "<br>additive smoothing: In statistics, additive smoothing is a technique to smooth categorical data. additive smoothing is introduced to solve the problem of zero probability. for applying it we add 1 to the count of all words in the train data, so when a word has not accured in the train its probability will not be zero as its count is now 1. zero probability will not accur and cases like above will be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_smoothing(rec_data,not_rec_data):\n",
    "    rec = {}\n",
    "    not_rec = {}\n",
    "    for i in rec_data:\n",
    "        rec[i] = rec_data[i] + 1\n",
    "    for i in not_rec_data:\n",
    "        not_rec[i] = not_rec_data[i] + 1\n",
    "    return rec, not_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) ‫‪evidence: P(comment) is the likelihood. we ignore it in this project!\n",
    "<br>2) likelihood: P(comment|recommended (or not recommended)) is the likelihood. we calculate it by multiplying all P(x|recommended (or not recommended))s, where x is each word used in comment.\n",
    "<br>3) prior: P(recommended (or not recommended)) is the prior probability of class. we calculate it by counting all of the recommended (or not recommended) comments in test data and devide it by number of all comments.\n",
    "<br>4) ‪posterior:P(recommended(or not recommended)|comment) is the posterior probability. it is the probability of being recommended or not recommended if the comment is what it is. we calculate it by multiplying likelihood and prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict(dic):\n",
    "    sum = 0\n",
    "    for i in dic: \n",
    "        sum = sum + dic[i]\n",
    "    return sum\n",
    "\n",
    "def rec_or_not(test_,rec_data,not_rec_data,flag):\n",
    "    rec_words_count = sum_dict(rec_data)\n",
    "    not_rec_words_count = sum_dict(not_rec_data)\n",
    "    for index,row in test_.iterrows():\n",
    "        rec_prob = recommend_count/(recommend_count+not_recommend_count)\n",
    "        not_rec_prob = not_recommend_count/(recommend_count+not_recommend_count)\n",
    "        if flag == 1: temp = normalize_str(row['title'] + ' ' + row['comment'])\n",
    "        else: temp = word_tokenize(row['title'] + ' ' + row['comment'])\n",
    "        for i in temp:\n",
    "            rec_prob *= (rec_data[i]/rec_words_count)\n",
    "            not_rec_prob *= (not_rec_data[i]/not_rec_words_count)\n",
    "            if rec_prob >= not_rec_prob: row['recommend'] = 'recommended'\n",
    "            else: row['recommend'] = 'not_recommended'\n",
    "    return test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "presicion: when we want to calculate whether our way for guessing the category is accurate or not we divide all the right \"recommended\" categoriy guesses by all of of the \"recommended\" guesses even if they are wrong. we may just find one of the comments recommended and it may be true. in this case our accuracy will be 1 and we may be 100% right but in fact we are far from the right answer.\n",
    "<br><br>recall: when we want to calculate whether our way for guessing the category is accurate or not we divide all the right \"recommended\" categoriy guesses by all of of the \"recommended\" categories which are the actual answer. in this way if we found that all of the comments are in the \"recommended\" category the above way for finding the accuracy will be one again as we know for sure that its not true.\n",
    "<br><br> for F1 we use harmonic mean. The harmonic mean helps to find multiplicative or divisor relationships between fractions without worrying about common denominators. Harmonic means are often used in averaging things like rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test):\n",
    "    wrong = []\n",
    "    right = 0\n",
    "    for i in range(test.count()['title']):\n",
    "        if test['recommend'][i] == test_data['recommend'][i] : right += 1\n",
    "        else: wrong.append((test['title'][i],test['comment'][i],test['recommend'][i]))\n",
    "    return right/test.count()['title'],wrong\n",
    "\n",
    "def precision(test):\n",
    "    right = 0\n",
    "    for i in range(test.count()['title']):\n",
    "        if test['recommend'][i] == test_data['recommend'][i] and test['recommend'][i] == 'recommended': right += 1\n",
    "    return right/(test[(test['recommend'] == 'recommended')].count()['title'])\n",
    "\n",
    "def recall(test):\n",
    "    right = 0\n",
    "    for i in range(test.count()['title']):\n",
    "        if test['recommend'][i] == test_data['recommend'][i] and test['recommend'][i] == 'recommended': right += 1\n",
    "    return right/(test_data[(test_data['recommend'] == 'recommended')].count()['title'])\n",
    "\n",
    "def f1(pre,rec):\n",
    "    return (2*pre*rec)/(pre+rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we see in the answers below from a to d step by step the percentage of correctness of our answer decreases. the reason is step by step we are not doing the things that helps us to find the correct answer. like in step b we are not using preprocceses and it is certainly helpful as we said in the above. or in c and d we are not using additive smoothing method and as we expected, our answer will be less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre process and additive smoothing:\n",
      "accuracy :  91.75 %\n",
      "precision:  89.38679 %\n",
      "recall :  94.75 %\n",
      "F1 :  91.99029 %\n"
     ]
    }
   ],
   "source": [
    "#pre process and additive smoothing\n",
    "rec_data_1,not_rec_data_1,test_1 = normalize_data(1)\n",
    "\n",
    "both_rec, both_not_rec = additive_smoothing(rec_data_1,not_rec_data_1)\n",
    "\n",
    "filled_test_both = rec_or_not(test_1,both_rec,both_not_rec,1)\n",
    "\n",
    "acc_1,wrong = accuracy(filled_test_both)\n",
    "prec_1 = precision(filled_test_both)*100\n",
    "rec_1 = recall(filled_test_both)*100\n",
    "f1_1 = f1(prec_1,rec_1)\n",
    "\n",
    "print(\"pre process and additive smoothing:\")\n",
    "print (\"accuracy : \",acc_1*100,\"%\")\n",
    "print (\"precision: \",'{:.5f}'.format(prec_1),\"%\")\n",
    "print (\"recall : \",rec_1,\"%\")\n",
    "print (\"F1 : \",'{:.5f}'.format(f1_1),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additive smoothing:\n",
      "accuracy :  91.375 %\n",
      "precision:  88.57809 %\n",
      "recall :  95.0 %\n",
      "F1 :  91.67672 %\n"
     ]
    }
   ],
   "source": [
    "#additive smoothing\n",
    "rec_data_2,not_rec_data_2,test_2 = normalize_data(0)\n",
    "additive_rec, additive_not_rec = additive_smoothing(rec_data_2,not_rec_data_2)\n",
    "filled_test_additive = rec_or_not(test_2,additive_rec,additive_not_rec,0)\n",
    "\n",
    "acc_2,r = accuracy(filled_test_additive)\n",
    "prec_2 = precision(filled_test_additive)*100\n",
    "rec_2 = recall(filled_test_additive)*100\n",
    "f1_2 = f1(prec_2,rec_2)\n",
    "\n",
    "print(\"additive smoothing:\")\n",
    "print (\"accuracy : \",acc_2*100,\"%\")\n",
    "print (\"precision: \",'{:.5f}'.format(prec_2),\"%\")\n",
    "print (\"recall : \",rec_2,\"%\")\n",
    "print (\"F1 : \",'{:.5f}'.format(f1_2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre process:\n",
      "accuracy :  86.0 %\n",
      "precision:  79.50820 %\n",
      "recall :  97.0 %\n",
      "F1 :  87.38739 %\n"
     ]
    }
   ],
   "source": [
    "#pre process\n",
    "rec_data_3,not_rec_data_3,test_3 = normalize_data(1)\n",
    "\n",
    "filled_test_pre = rec_or_not(test_3,rec_data_3,not_rec_data_3,1)\n",
    "\n",
    "acc_3,r = accuracy(filled_test_pre)\n",
    "prec_3 = precision(filled_test_pre)*100\n",
    "rec_3 = recall(filled_test_pre)*100\n",
    "f1_3 = f1(prec_3,rec_3)\n",
    "\n",
    "print(\"pre process:\")\n",
    "print (\"accuracy : \",acc_3*100,\"%\")\n",
    "print (\"precision: \",'{:.5f}'.format(prec_3),\"%\")\n",
    "print (\"recall : \",rec_3,\"%\")\n",
    "print (\"F1 : \",'{:.5f}'.format(f1_3),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none:\n",
      "accuracy :  85.625 %\n",
      "precision:  78.443 %\n",
      "recall :  98.25 %\n",
      "F1 :  87.23640 %\n"
     ]
    }
   ],
   "source": [
    "#none\n",
    "rec_data_4,not_rec_data_4,test_4 = normalize_data(0)\n",
    "\n",
    "filled_test_none = rec_or_not(test_4,rec_data_4,not_rec_data_4,0)\n",
    "\n",
    "acc_4,r = accuracy(filled_test_none)\n",
    "prec_4 = precision(filled_test_none)*100\n",
    "rec_4 = recall(filled_test_none)*100\n",
    "f1_4 = f1(prec_4,rec_4)\n",
    "\n",
    "print(\"none:\")\n",
    "print (\"accuracy : \",acc_4*100,\"%\")\n",
    "print (\"precision: \",'{:.5}'.format(prec_4),\"%\")\n",
    "print (\"recall : \",rec_4,\"%\")\n",
    "print (\"F1 : \",'{:.5f}'.format(f1_4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will discuss one of the comments which was expected to be \"not recommended\" but is is \"recommended\":\n",
    "<br><br>'برد بالایی نداره کیفیت صداش معمولیه برای تایم تمرین و ورزش مناسبه از روی گوش نمیفته  ظریف نیست میان نمونه های هم رده خودش خیلی خوب نیست'\n",
    "<br><br>as we can see in this comment good attributes are used with negetive verb. as verbs are less than the attributes they are less likely to make an impact. we could use weighted probability which verbs weight could be more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>وری گود</td>\n",
       "      <td>تازه خریدم یه مدت کار بکنه مشخص میشه کیفیت قطعاتش</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>دستگاه خیلی ضعیف</td>\n",
       "      <td>من این فیس براس چند روز یپش به دستم رسید و الا...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>خوب ولی کارایی محدود</td>\n",
       "      <td>مدل 46MM به دست شما نخواهد رسید و به جای آن مد...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نقد پس از خرید</td>\n",
       "      <td>سلام ، راحت شدم از کابل شارژ ، توصیه میشود به ...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نقد منصفانه</td>\n",
       "      <td>من تو تخفیف ویژه 5 تا خریدم و همشون رو هم تست ...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>عدم بسته بندی و تحویل مناسب</td>\n",
       "      <td>خیلی نردبان خوبیه خیلی بدرد بخوره تنها نکته من...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ظاهر خراب</td>\n",
       "      <td>شکل و ظاهر محصول که خیلی خط و خش داشت و پایینش...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>پیشنهاد نمیدم</td>\n",
       "      <td>من دوسه ماهی هست این کفشدازردیجی گرفتم متاسفان...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>SanDisk</td>\n",
       "      <td>سلام وقتتون بخیر\\r\\nاولش که من این USB رو خرید...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>کلنیل</td>\n",
       "      <td>این نوعش به درد نمیخوره نوعش که گیاهیه خوبه.من...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  \\\n",
       "0                       وری گود   \n",
       "1              دستگاه خیلی ضعیف   \n",
       "2          خوب ولی کارایی محدود   \n",
       "3                نقد پس از خرید   \n",
       "4                   نقد منصفانه   \n",
       "..                          ...   \n",
       "61  عدم بسته بندی و تحویل مناسب   \n",
       "62                    ظاهر خراب   \n",
       "63                پیشنهاد نمیدم   \n",
       "64                      SanDisk   \n",
       "65                        کلنیل   \n",
       "\n",
       "                                              comment        recommend  \n",
       "0   تازه خریدم یه مدت کار بکنه مشخص میشه کیفیت قطعاتش  not_recommended  \n",
       "1   من این فیس براس چند روز یپش به دستم رسید و الا...      recommended  \n",
       "2   مدل 46MM به دست شما نخواهد رسید و به جای آن مد...      recommended  \n",
       "3   سلام ، راحت شدم از کابل شارژ ، توصیه میشود به ...  not_recommended  \n",
       "4   من تو تخفیف ویژه 5 تا خریدم و همشون رو هم تست ...      recommended  \n",
       "..                                                ...              ...  \n",
       "61  خیلی نردبان خوبیه خیلی بدرد بخوره تنها نکته من...  not_recommended  \n",
       "62  شکل و ظاهر محصول که خیلی خط و خش داشت و پایینش...      recommended  \n",
       "63  من دوسه ماهی هست این کفشدازردیجی گرفتم متاسفان...  not_recommended  \n",
       "64  سلام وقتتون بخیر\\r\\nاولش که من این USB رو خرید...      recommended  \n",
       "65  این نوعش به درد نمیخوره نوعش که گیاهیه خوبه.من...      recommended  \n",
       "\n",
       "[66 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(wrong, columns =['title', 'comment', 'recommend']) \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
